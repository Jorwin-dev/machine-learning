{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification using Tensorflow and Keras by developing Feed-Forward Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First of all, what is a Neural Network?\n",
    "-  A **neural network** is a computational model inspired by the structure of the human brain, consisting of layers of interconnected nodes (neurons) that process and learn patterns from data to perform tasks like classification, prediction, and recognition.\n",
    "- They are often used in applications such as image recognition, natural language processing, recommendation systems, and generative AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Definitions\n",
    "- **TensorFlow:** An open-source machine learning framework developed by Google that provides a flexible platform for building and deploying deep learning models.\n",
    "- **Keras:** A high-level neural network API built on top of TensorFlow, designed for easy and fast prototyping of deep learning models.\n",
    "- **Feed-Forward Neural Network:** A type of artificial neural network where information moves in one direction—from input to output—without loops or cycles, making it the simplest form of deep learning architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement: Predicting 'red wine quality'. \n",
    "- If Quality >= 5.5, then it is **'good wine'** (1).\n",
    "- Otherwise it is **'bad wine'** (0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Step: Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key question before pre-processing is how many rows and columns does the data have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the dataset, skipping the first row (headers) and selecting only numerical columns\n",
    "dataset = np.loadtxt('WineQT.csv', delimiter=',', skiprows=1, usecols=range(12))\n",
    "\n",
    "# Print dataset dimensions\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert dataset to a Pandas Dataframe for easier analysis\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "print(df.head())    # Display first 5 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset with headers for better column identification\n",
    "df = pd.read_csv('WineQT.csv')  \n",
    "df.head()   # Show first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'Id' column since it's useless for training\n",
    "df = df.drop(columns=['Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display data set info now\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the dataset back to a NumPy array because all calculations, preprocessing, and execution will be done using NumPy, but I initially used Pandas for easier inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Preview the first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints only floating point numbers with a 2 decimal point precision. \n",
    "np.set_printoptions(formatter={'float': lambda x: '{0:0.2f}'.format(x)})\n",
    "\n",
    "print(dataset[0:5, :])  # Display rows 0 to x with all columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the last column is less than 5.5, set it to 0, otherwise 1.\n",
    "- (good wine = 1, bad wine = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset[:, -1] < 5.5, -1] = 0\n",
    "dataset[dataset[:, -1] >= 5.5, -1] = 1\n",
    "\n",
    "print(dataset[0:20, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now converted this problem into a **binary classification** problem because our output labels are now 0's and 1's. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Shuffle the rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is done simply to get a good mix of all of the rows. It may be bad for reproducibility, but it is good for reliability. Why?\n",
    "- every time you shuffle you will get completely different results, but it more or less guarantees that you won't get all 0's in the training or 1's in the testing and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "np.random.shuffle(dataset) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Split into Training/Testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the input values that go into the model are Xtrain, and what the model needs to predict is Ytrain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proposed rule of thumb is an 80/20 split for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_20percent = int(0.2 * len(dataset[:, 0]))\n",
    "\n",
    "print(index_20percent)  # 228 samples will be tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take samples from start to 228 for Testing\n",
    "XTEST = dataset[:index_20percent, :-1]  # Extract all columns except the last into X (indicated by :-1)\n",
    "YTEST = dataset[:index_20percent, -1]   # Extract last column only into y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take samples from 228 onwards (: indicates this)\n",
    "XTRAIN = dataset[index_20percent:, 0:-1]    # Extract all columns besides last\n",
    "YTRAIN = dataset[index_20percent:, -1]       # Extract only last"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Normalize the data (if needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all datasets need normalization, but if your input column values do not necessarily sit around 0-1 then you may need to normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization is needed when input values vary widely in scale because neural networks learn best when data is within a small, consistent range; large differences between features can slow training, cause unstable gradients, and make certain features dominate learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common practice to normalize is to use standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(XTRAIN[:, 0])\n",
    "plt.ylabel('0th Column (fixed acidity)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 0th column 'fixed acidity has values ranging from roughly 3-15, and most are in the 6-10 region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, since these values are not in the 0-1 range we want to normalize them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the split of 0's and 1's in our Y-data, aka what we are predicting.\n",
    "\n",
    "plt.hist(YTRAIN)\n",
    "plt.ylabel('Output labels')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(YTEST)\n",
    "plt.ylabel('Output labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline Accuracy** is determined by this split, as if you have more of one value than the other the baseline accuracy will be greater than or less than 50%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets normalize the data with standardization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You obtain the mean and standard deviation from your training data\n",
    "# Then use those values to normalize your testing data\n",
    "\n",
    "mean = XTRAIN.mean(axis=0)\n",
    "XTRAIN -= mean\n",
    "std = XTRAIN.std(axis=0)\n",
    "XTRAIN /= std\n",
    "\n",
    "XTEST -= mean\n",
    "XTRAIN /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to calculate the mean and std using the training data, and subsequently use the same values for the testing data. Why?\n",
    "- You only use the training data to calculate mean and std as a **True Test** of the model how it does on testing data using the parameters learned from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean and std are vectors\n",
    "# So, we can see the mean and std for each feature we have\n",
    "\n",
    "print(mean)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, look at the distribution of values for the same column again. \n",
    "\n",
    "plt.hist(XTRAIN[:, 0])\n",
    "plt.ylabel('0th Column (fixed acidity)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Review the Dimensions of the training & testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also previews some of the 'input features' and 'correct labels' for the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of rows in XTRAIN & YTRAIN must be the same.\n",
    "print(XTRAIN.shape)\n",
    "print(YTRAIN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same goes for the testing sets.\n",
    "print(XTEST.shape)\n",
    "print(YTEST.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the head (first 3) of the datasets\n",
    "print(XTRAIN[0:3, ])\n",
    "print(YTRAIN[0:3])\n",
    "print(XTEST[0:3, ])\n",
    "print(YTEST[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create a neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create a network with the following architecture:\n",
    "- 8 neurons in layer 1\n",
    "- 4 neurons in layer 2\n",
    "- 1 neuron as the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential  # Sequential model to stack layers\n",
    "from tensorflow.keras.layers import Dense       # Dense layer aka fully connected layer\n",
    "\n",
    "# Initialize a Sequential model (stacks layers in order)\n",
    "model = Sequential()\n",
    "\n",
    "# The number of inputs that each neuron receives is the number of columns in the data\n",
    "model.add(Dense(8, input_dim = len(XTRAIN[0, :]), activation='relu'))\n",
    "\n",
    "# input_dim = len(XTRAIN[0, :]) calculates the length by using the first row (0) and all columns(:)\n",
    "# Necessary for first layer as each neuron needs to know how many inputs it is receiving when designing the architecture of the NN.\n",
    "\n",
    "# All of the outputs from the 8 neurons in layer 1 become inputs to the 4 neurons in layer 2\n",
    "model.add(Dense(4, activation='relu'))\n",
    "\n",
    "# Since we are doing binary classification, we want 1 neuron in the last layer.\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why ReLU and Sigmoid?\n",
    "- **ReLU (`relu`)** is used in hidden layers to introduce non-linearity. It helps the model learn complex patterns and prevents the vanishing gradient problem.\n",
    "- **Sigmoid (`sigmoid`)** is used in the output layer for binary classification. It converts raw scores (logits) into probabilities between 0 and 1.\n",
    "- **Logits** are the raw, unscaled outputs of a neuron before applying an activation function. They can take any real value, but we apply an activation function (like `sigmoid`) to convert them into interpretable probabilities.\n",
    "- **The Vanishing Gradient Problem** happens when the learning signal (gradient) becomes too small, making earlier layers in a deep network stop learning. This is like trying to learn from feedback that gets quieter and quieter until you can’t hear it anymore. ReLU helps fix this by keeping gradients strong and allowing deeper layers to keep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model architecture\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `model.summary()` function provides a structured overview of the neural network, showing key details about each layer:\n",
    "\n",
    "- **Layer Type**: Lists the different layers in the model (e.g., `Dense` for fully connected layers).  \n",
    "- **Output Shape**: Displays the shape of the data as it moves through each layer.  \n",
    "  - `(None, 8)`, `(None, 4)`, `(None, 1)` → The `None` indicates that the model can process any batch size.  \n",
    "  - The numbers (8, 4, 1) represent the number of neurons in each layer.  \n",
    "- **Number of Parameters**: Shows how many **trainable weights (connections between neurons) and biases (offset values for neurons)** each layer contains. For example, the first layer has **96 parameters** because 11*8 (weights) = 88 + 8 (biases) = 96.\n",
    "- **Total Parameters**: The sum of all layer parameters, indicating the complexity of the model.  \n",
    "- **Trainable Parameters**: The parameters that are updated during training.  \n",
    "- **Non-Trainable Parameters**: Parameters that are fixed (e.g., from a pre-trained model).\n",
    "\n",
    "### **Why This is Useful**\n",
    "- Helps ensure the architecture is **correctly structured** before training.  \n",
    "- Provides insight into the **model’s complexity** (more parameters = more capacity, but also risk of overfitting).  \n",
    "- Useful for **debugging** if unexpected shapes or parameter mismatches occur.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "img = mpimg.imread('NN.png')\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the input features go into the network in a different way than the output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 11 different features (columns) go into the 8 neurons in the 1st layer, from each column to every neuron. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those 8 neurons then feed the 4 neurons, and those 4 feed the final neuron that sits at the end. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That final neuron, based on all the previous weights and calculations gives you an accuracy score from 0 to 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the score is bad, then the weights that connect the nodes need to be updated such that in future rounds of training the output matches closer with the input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Compile the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This checks if there are cycles in the **sequential model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, when designing sequential models there are usually no errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When defining the loss, for binary classification **binary crossentropy** is the only choice. Why?\n",
    "- **Binary crossentropy** is used for binary classification because it measures how well the predicted probabilities match the actual class labels, making it ideal for problems with two possible outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For optimizer, we chose *adam* but could have chosen any adaptive optimizer like *rmsprop*.\n",
    "- Adaptive optimizers automatically adjust the learning rate during training, allowing for faster and more stable convergence compared to fixed learning rate optimizers like SGD.\n",
    "- **Adam (Adaptive Moment Estimation)** is an optimizer that combines the advantages of both RMSprop and momentum, making it effective for most deep learning tasks.\n",
    "- **RMSprop (Root Mean Square Propagation)** is another optimizer that adapts the learning rate for each parameter, preventing large updates that can slow down convergence.\n",
    "- **Momentum Optimizer** accelerates training by allowing the optimizer to keep moving in the same direction even if gradients change slightly, helping escape small local minima and improving convergence speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For **binary classification** the most widely used metric is *accuracy* as it is easier to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We feed *XTRAIN* into the model and the model calculates errors using *YTRAIN*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In one *epoch* the model scans through all the rows in *XTRAIN*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An **epoch** is one full pass through of the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating the number of *epochs* usually increases the accuracy of the model, as the more often that the training process sees the entire dataset the more likely it is to get better on it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add *test_data = (XTEST, YTEST)* to observe the accuracy of *TEST* data during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some callbacks\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "callback_a = ModelCheckpoint(filepath = 'my_best_model.keras', monitor='val_loss', save_best_only = True)   # Prevents saving worse models\n",
    "callback_b = EarlyStopping(monitor='val_loss', mode='min', patience=20, verbose=1)                          # Prevents overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks: ModelCheckpoint & EarlyStopping\n",
    "We added them to improve the model's training efficiency.\n",
    "- **Model Checkpoint** saves the best model based on validation loss.\n",
    "- **Early Stopping** stops the model if the validation loss does not decrease for 20 epochs. \n",
    "- **Validation Loss** measures how well the model performs on unseen data. A low validation loss means the model is learning patterns that generalize well, while a high validation loss may indicate overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(XTRAIN, YTRAIN, validation_data=(XTEST, YTEST), epochs=256, batch_size=10, callbacks=[callback_a, callback_b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Check the learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The learning curves are the loss, accuracy, etc... over the number of epochs.\n",
    "- Learning curves can only be tracked if we used history when training the model, as that is what stores the output of the model during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training_data', 'validation_data'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The training accuracy steadily increased, meaning the model is learning the training data well. \n",
    "- The validation accuracy increased, but flattens and slightly fluctuated, meaning that after some epochs, the model stopped improving on unseen data. \n",
    "- The dip in validation accuracy happens because the model initially struggles to generalize, possibly due to a high learning rate, small batch size, or overfitting to early training examples before stabilizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests overfitting, meaning the model memorized the training data instead of generalizing well to new data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload the best weights so we are back with the highest performing model on the testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('my_best_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Evaluate the model on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is done to validate the results we got on the Testing set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are evaluating the model on the same data we used to train it, so essentially this evaluation is meaningless. But, it is done to make sure everything is working okay. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(XTRAIN, YTRAIN)\n",
    "print(model.metrics_names)\n",
    "print(scores)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displays the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Evaluate on testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a **real test** on the model as we are evaluating it on the 'unknown' dataset, aka unseen data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(XTEST, YTEST)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lower than the training evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Check what the model actually predicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of what the model has predicted and a subsequent comparison with the true classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(XTEST[0:5])\n",
    "print(YTEST[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(XTEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prediction[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the confidence scores are not matching in some areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prediction[0:10].round())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is clearer once we round. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following scatter plot visualizes the relationship between the **true labels (X-axis)** and the **predicted confidence scores (Y-axis)**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(YTEST, prediction, '.', alpha=0.3)\n",
    "plt.xlabel('Correct labels')\n",
    "plt.ylabel('Predicted confidence scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each point represents a **single prediction** made by the model.  \n",
    "- The X-axis (Correct labels) shows the actual class labels **(0 = bad wine, 1 = good wine)**.  \n",
    "- The Y-axis (Predicted confidence scores) shows the **model’s confidence** in classifying the sample as **good wine (1)**.  \n",
    "\n",
    "**Interpreting the Graph:**  \n",
    "- **Points close to 0 or 1 on the Y-axis** = The model is confident in its predictions.  \n",
    "- **Points near 0.5** = The model is uncertain, meaning the classification is less reliable.  \n",
    "- **Ideally**, predictions for class **0** should cluster near the bottom (low confidence), and predictions for class **1** should cluster near the top (high confidence).  \n",
    "- **If many points are incorrectly positioned**, the model may struggle with classification, possibly due to overfitting or imbalanced data.  \n",
    "\n",
    "This visualization helps identify patterns in model confidence and potential areas for improvement.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Question: Is 'accuracy' sufficient enough to evaluate our model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many times, accuracy may not be the best metric to evaluate what our model is predicting. **Accuracy can be misleading** if the dataset is **imbalanced** (ex: predicting 99% \"negative\" in a dataset where 99% of cases are negative still gives high accuracy but fails to detect positives)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further assess the model's performance, we used the following metrics:\n",
    "\n",
    "- **Accuracy**: Measures the percentage of correct predictions.  \n",
    "  - **High** = The model is making mostly correct predictions.  \n",
    "  - **Low** = The model is performing poorly, possibly just guessing.\n",
    "\n",
    "- **Precision**: Measures how often a predicted positive is actually correct.  \n",
    "  - **High** = Few false positives, reliable positive predictions.  \n",
    "  - **Low** = Many false positives, model predicts positives incorrectly too often.  \n",
    "\n",
    "- **Recall (Sensitivity)**: Measures how well the model detects actual positives.  \n",
    "  - **High** = Few false negatives, model catches most actual positives.  \n",
    "  - **Low** = Many false negatives, missing actual positive cases.  \n",
    "\n",
    "- **F1-Score**: A balance between precision and recall.  \n",
    "  - **High** = Both precision and recall are strong.  \n",
    "  - **Low** = One or both metrics are weak, indicating poor overall performance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(YTEST, prediction.round())\n",
    "precision = precision_score(YTEST, prediction.round())\n",
    "recall = recall_score(YTEST, prediction.round())\n",
    "f1score = f1_score(YTEST, prediction.round())\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(\"Precision: %.2f%%\" % (precision * 100.0))\n",
    "print(\"Recall: %.2f%%\" % (recall * 100.0))\n",
    "print(\"F1-score: %.2f\" % (f1score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Question: How can the performance be improved?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we could increase the number of epochs to 100 or 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## **Testing Improvement 1: Increasing Epochs**\n",
    "# Increase the number of epochs to **100** and observe if the model improves.\n",
    "\n",
    "model.fit(XTRAIN, YTRAIN, validation_data=(XTEST, YTEST), epochs=100, batch_size=32, callbacks=[callback_a])\n",
    "\n",
    "# Display final accuracy after increasing epochs\n",
    "print(f\"Final Training Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "# Plot accuracy over 100 epochs\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy After Increasing Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we could add more layers to the neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## **Testing Improvement 2: Adding More Layers**\n",
    "# We'll add an additional hidden layer (making it a **4-layer network**) and see if performance improves.\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define new model with an extra hidden layer\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(8, input_dim=len(XTRAIN[0, :]), activation='relu'))\n",
    "model2.add(Dense(6, activation='relu'))  # New layer added\n",
    "model2.add(Dense(4, activation='relu'))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the new model\n",
    "history2 = model2.fit(XTRAIN, YTRAIN, validation_data=(XTEST, YTEST), epochs=50, batch_size=32, callbacks=[callback_a, callback_b])\n",
    "\n",
    "# Display final accuracy after increasing epochs\n",
    "print(f\"Final Training Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "# Plot accuracy over 100 epochs\n",
    "plt.plot(history2.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history2.history['val_accuracy'], label='Validation Accuracy')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy After Increasing Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also balance the data, meaning create a good balance of positive and negative class values so the evaluation makes sense "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## **Testing Improvement 3: Balancing the Data**\n",
    "# We'll use **random oversampling** to balance the classes and see if it helps improve model performance.\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Apply oversampling to balance the dataset\n",
    "ros = RandomOverSampler()\n",
    "XTRAIN_resampled, YTRAIN_resampled = ros.fit_resample(XTRAIN, YTRAIN)\n",
    "\n",
    "# Train model on the balanced dataset\n",
    "history3 = model.fit(XTRAIN_resampled, YTRAIN_resampled, validation_data=(XTEST, YTEST), epochs=50, batch_size=32, callbacks=[callback_a, callback_b])\n",
    "\n",
    "# Display final accuracy after increasing epochs\n",
    "print(f\"Final Training Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "# Plot accuracy over 100 epochs\n",
    "plt.plot(history3.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history3.history['val_accuracy'], label='Validation Accuracy')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy After Increasing Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we could increase/decrease the number of rows in the training/testing set, meaning alter the split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## **Testing Improvement 4: Adjusting Train/Test Split**\n",
    "# We'll change the train/test split to **90% train, 10% test** and check its effect.\n",
    "\n",
    "# Adjust train/test split to 90% train, 10% test\n",
    "index_10percent = int(0.1 * len(dataset[:, 0]))\n",
    "\n",
    "XTEST_new = dataset[:index_10percent, :-1]\n",
    "YTEST_new = dataset[:index_10percent, -1]\n",
    "XTRAIN_new = dataset[index_10percent:, :-1]\n",
    "YTRAIN_new = dataset[index_10percent:, -1]\n",
    "\n",
    "# Train model with new split\n",
    "history4 = model.fit(XTRAIN_new, YTRAIN_new, validation_data=(XTEST_new, YTEST_new), epochs=50, batch_size=32, callbacks=[callback_a, callback_b])\n",
    "\n",
    "# Display final accuracy after increasing epochs\n",
    "print(f\"Final Training Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "# Plot accuracy over 100 epochs\n",
    "plt.plot(history4.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history4.history['val_accuracy'], label='Validation Accuracy')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy After Increasing Epochs')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
